

# OpenAI Configuration
OPENAI_BASE_URL=https://api.openai.com/v1

OPENAI_API_KEY=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
OPENAI_MODEL=gpt-4o-mini



# Database Configuration
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/aidb

# Vector Database (Qdrant)
# For Docker: use container name 'qdrant'
# For local development: use 'localhost'
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION=assessment_docs_minilm
QDRANT_SEED_PATH=/app/data/qdrant_seed/assessment_docs_minilm.jsonl
RAG_VECTOR_SIZE=384
QDRANT_SEED_VECTOR_SIZE=384
# Parallel seeding configuration (default: 4 workers, 200 vectors/batch)
QDRANT_SEED_MAX_WORKERS=4
QDRANT_SEED_BATCH_SIZE=200

# ONNX Inference Service
USE_ONNX_INFERENCE=true
USE_INT8_QUANTIZATION=true
# Adaptive Model Selection: MiniLM (primary) and BGE (fallback)
# SHORT-TERM STRATEGY: MiniLM primary for compatibility with existing Qdrant data
# - MiniLM models: Fast inference, compatible with assessment_docs_minilm collection
# - BGE models: High accuracy fallback for complex queries (requires re-ingestion to use as primary)
ONNX_EMBED_MODEL_PATH=./models/minilm-embed-int8
ONNX_RERANK_MODEL_PATH=./models/bge-reranker-int8
EMBED_FALLBACK_MODEL_PATH=./models/bge-m3-embed-int8
RERANK_FALLBACK_MODEL_PATH=./models/minilm-reranker-onnx
INFERENCE_SERVICE_URL=http://localhost:8001

# Performance Configuration
OMP_NUM_THREADS=6
LOG_LEVEL=INFO

# Optional: Reranker CPU performance threshold (switch to fallback if slower)
RERANK_CPU_SWITCH_THRESHOLD_MS=500
# Reranker score threshold - filter out results below this score
# Default -20.0 to allow more results (reranker may score relevant docs low for complex queries)
# The LLM will filter out irrelevant results based on content
RERANK_SCORE_THRESHOLD=-20.0

# Optional: External APIs for Agent (Task 3.3)
WEATHER_API_KEY=optional-weather-api-key
FLIGHTS_API_KEY=optional-flights-api-key

# === Advanced RAG Features (Phase 1) ===
# Hybrid Search: Combine BM25 keyword search with dense vector search
ENABLE_HYBRID_SEARCH=true
HYBRID_ALPHA=0.7  # Weight for vector search (0-1). 0.7 = 70% vector, 30% BM25
BM25_TOP_K=25  # Number of BM25 candidates to retrieve

# Query Strategy Cache: Cache successful retrieval strategies for 90% token savings
ENABLE_QUERY_CACHE=true
QUERY_CACHE_SIMILARITY_THRESHOLD=0.85  # Cosine similarity threshold for query matching (0-1)
QUERY_CACHE_MAX_SIZE=1000  # Maximum number of cached queries (LRU eviction)
QUERY_CACHE_TTL_HOURS=24  # Time to live for cached entries in hours

# Query Classification: Optimize retrieval parameters based on query type
ENABLE_QUERY_CLASSIFICATION=true

# Answer Cache (Multi-Layer Hybrid): Cache complete answers for maximum token savings
ENABLE_ANSWER_CACHE=true
ANSWER_CACHE_SIMILARITY_THRESHOLD=0.88  # Layer 3: Semantic similarity threshold (0.85-0.92 recommended)
ANSWER_CACHE_TFIDF_THRESHOLD=0.30  # Layer 2: TF-IDF keyword threshold (0.25-0.35 recommended)
ANSWER_CACHE_MAX_SIZE=1000  # Maximum cached answers (LRU eviction)
ANSWER_CACHE_TTL_HOURS=72  # Answer cache TTL (72 hours = 3 days)

# === Advanced RAG Features (Phase 2) ===
# Self-RAG: Iterative retrieval with confidence thresholds
ENABLE_SELF_RAG=true
SELF_RAG_CONFIDENCE_THRESHOLD=0.75  # Minimum confidence to stop iterating (0-1)
SELF_RAG_MAX_ITERATIONS=3  # Maximum number of retrieval iterations
SELF_RAG_MIN_IMPROVEMENT=0.05  # Minimum confidence improvement to continue (0-1)
# Graph RAG JIT tuning
GRAPH_JIT_MAX_CHUNKS=10
GRAPH_JIT_BATCH_SIZE=4
GRAPH_JIT_BATCH_TIMEOUT=30

# === File-Level BGE Fallback (Phase 3) ===
# Confidence-based BGE fallback: MiniLM as file finder (fast), BGE as chunk locator (accurate)
ENABLE_FILE_LEVEL_FALLBACK=true
CONFIDENCE_FALLBACK_THRESHOLD=0.65  # Score below which to trigger BGE file-level re-embedding (0-1)
FILE_FALLBACK_CHUNK_SIZE=500  # Chunk size for BGE re-chunking
FILE_FALLBACK_CHUNK_OVERLAP=50  # Chunk overlap for BGE re-chunking

# === Thompson Sampling Bandit Persistence ===
# Bandit state file path - persistent across restarts
BANDIT_STATE_FILE=./cache/smart_bandit_state.json
