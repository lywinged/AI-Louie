# ç­”æ¡ˆç¼“å­˜é›†æˆæŒ‡å— - å¤ç”¨ç°æœ‰ BGE-M3 æ¨¡å‹ âœ…

## æ ¸å¿ƒè¦ç‚¹

âœ… **ä¸éœ€è¦æ–°æ¨¡å‹**ï¼šç›´æ¥å¤ç”¨ä½ ç°æœ‰çš„ BGE-M3 embedding æ¨¡å‹
âœ… **é›¶é¢å¤–å¼€é”€**ï¼šä¸å¢åŠ å†…å­˜ï¼Œä¸åŠ è½½æ–°æ¨¡å‹
âœ… **å®Œç¾å…¼å®¹**ï¼šä¸ RAG ä½¿ç”¨ç›¸åŒçš„è¯­ä¹‰ç©ºé—´

---

## ä½ å½“å‰çš„ Embedding æ¨¡å‹

```python
æ¨¡å‹åç§°: BAAI/bge-m3 (BGE-Multilingual-Embedding-3)
å‘é‡ç»´åº¦: 1024 ç»´ (æ¯” MiniLM-384 æ›´å¼ºå¤§)
å·²æœ‰å‡½æ•°: _embed_texts(texts: List[str]) â†’ List[List[float]]
ä½ç½®: backend/services/rag_pipeline.py:96-107
```

---

## é›†æˆæ­¥éª¤

### Step 1: åˆå§‹åŒ–ç­”æ¡ˆç¼“å­˜

åœ¨ `enhanced_rag_pipeline.py` é¡¶éƒ¨æ·»åŠ å…¨å±€å®ä¾‹ï¼š

```python
from backend.services.answer_cache import MultiLayerAnswerCache, get_answer_cache

# Global instance
_answer_cache: Optional[MultiLayerAnswerCache] = None


def _get_answer_cache() -> Optional[MultiLayerAnswerCache]:
    """Get or create answer cache instance"""
    global _answer_cache

    if not os.getenv("ENABLE_ANSWER_CACHE", "false").lower() == "true":
        return None

    if _answer_cache is None:
        try:
            from backend.services.answer_cache import initialize_answer_cache

            threshold = float(os.getenv("ANSWER_CACHE_SIMILARITY_THRESHOLD", "0.88"))
            tfidf_threshold = float(os.getenv("ANSWER_CACHE_TFIDF_THRESHOLD", "0.30"))
            max_size = int(os.getenv("ANSWER_CACHE_MAX_SIZE", "1000"))
            ttl_hours = int(os.getenv("ANSWER_CACHE_TTL_HOURS", "72"))

            _answer_cache = initialize_answer_cache(
                similarity_threshold=threshold,
                tfidf_threshold=tfidf_threshold,
                max_cache_size=max_size,
                ttl_hours=ttl_hours
            )

            # æ³¨å…¥ç°æœ‰çš„ embedding å‡½æ•°ï¼ˆå¤ç”¨ BGE-M3ï¼‰
            from backend.services.rag_pipeline import _embed_texts

            async def embed_single(text: str) -> List[float]:
                """åŒ…è£…ç°æœ‰ embedding å‡½æ•°ï¼Œä» batch è½¬ä¸º single"""
                return (await _embed_texts([text]))[0]

            _answer_cache.set_embedder(embed_single)

            logger.info(
                "Answer cache initialized with BGE-M3",
                semantic_threshold=threshold,
                tfidf_threshold=tfidf_threshold,
                max_size=max_size
            )
        except Exception as e:
            logger.error("Failed to initialize answer cache", error=str(e))
            return None

    return _answer_cache
```

### Step 2: åœ¨ RAG pipeline å¼€å§‹å¤„æ£€æŸ¥ç¼“å­˜

åœ¨ `answer_question_hybrid()` å‡½æ•°å¼€å§‹æ·»åŠ ï¼š

```python
async def answer_question_hybrid(
    question: str,
    *,
    top_k: int = 5,
    use_llm: bool = True,
    include_timings: bool = True,
    reranker_override: Optional[str] = None,
    vector_limit: Optional[int] = None,
    content_char_limit: Optional[int] = None,
    use_cache: bool = True,
    use_classifier: bool = True,
) -> RAGResponse:
    tic_total = time.perf_counter()

    # === NEW: Check answer cache FIRST ===
    answer_cache = _get_answer_cache()
    if answer_cache and use_cache:
        try:
            cached = await answer_cache.find_cached_answer(question)
            if cached:
                logger.info(
                    "Answer cache HIT",
                    query=question[:50],
                    layer=cached['cache_layer'],
                    method=cached['cache_method'],
                    similarity=cached['similarity'],
                    time_ms=cached['time_ms']
                )
                # ç›´æ¥è¿”å›ç¼“å­˜çš„ç­”æ¡ˆï¼ˆçœæ‰æ‰€æœ‰ tokenï¼ï¼‰
                return cached['answer']
        except Exception as e:
            logger.warning("Answer cache lookup failed", error=str(e))

    # === åŸæœ‰æµç¨‹ç»§ç»­... ===
    cache = _get_query_cache() if use_cache else None
    classifier = _get_query_classifier() if use_classifier else None
    ...
```

### Step 3: åœ¨è¿”å›å‰ç¼“å­˜ç­”æ¡ˆ

åœ¨å‡½æ•°æœ«å°¾è¿”å›ç»“æœå‰æ·»åŠ ï¼š

```python
    # Build response
    response = RAGResponse(
        answer=answer,
        citations=citations,
        retrieval_time_ms=retrieval_ms,
        ...
    )

    # === NEW: Cache the answer ===
    if answer_cache and use_cache and llm_used:
        try:
            await answer_cache.cache_answer(
                query=question,
                rag_response=response,
                metadata={
                    'retrieval_ms': retrieval_ms,
                    'llm_ms': llm_time_ms,
                    'num_chunks': len(chunks)
                }
            )
        except Exception as e:
            logger.warning("Failed to cache answer", error=str(e))

    return response
```

---

## .env é…ç½®

åœ¨ä½ çš„ `.env` æ–‡ä»¶æ·»åŠ ï¼š

```bash
# ===== Answer Cache (Multi-Layer Hybrid) =====

# å¯ç”¨ç­”æ¡ˆç¼“å­˜ï¼ˆçœŸæ­£çœ 90% tokenï¼‰
ENABLE_ANSWER_CACHE=true

# Layer 3: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ (0.85-0.92 æ¨è)
# æ›´é«˜ = æ›´ä¸¥æ ¼ = æ›´å‡†ç¡®ä½†å‘½ä¸­ç‡ä½
# æ›´ä½ = æ›´å®½æ¾ = å‘½ä¸­ç‡é«˜ä½†å¯èƒ½è¯¯åˆ¤
ANSWER_CACHE_SIMILARITY_THRESHOLD=0.88

# Layer 2: TF-IDF å…³é”®è¯é˜ˆå€¼ (0.25-0.35 æ¨è)
ANSWER_CACHE_TFIDF_THRESHOLD=0.30

# æœ€å¤§ç¼“å­˜æ¡ç›®æ•° (LRU æ·˜æ±°)
ANSWER_CACHE_MAX_SIZE=1000

# ç¼“å­˜æœ‰æ•ˆæœŸ (å°æ—¶)
ANSWER_CACHE_TTL_HOURS=72
```

---

## BGE-M3 vs MiniLM å¯¹æ¯”

### ä¸ºä»€ä¹ˆ BGE-M3 æ›´å¥½ï¼Ÿ

| ç‰¹æ€§ | BGE-M3 (ä½ ç°åœ¨çš„) | MiniLM-L6 |
|------|-------------------|-----------|
| ç»´åº¦ | **1024** | 384 |
| è¯­ä¹‰ç†è§£ | â­â­â­â­â­ (æ›´å¼º) | â­â­â­â­ |
| å¤šè¯­è¨€ | âœ… æ”¯æŒ100+è¯­è¨€ | âŒ ä»…è‹±æ–‡ |
| å‡†ç¡®æ€§ | **æ›´é«˜** | ä¸€èˆ¬ |
| é€Ÿåº¦ | â­â­â­â­ | â­â­â­â­â­ (ç¨å¿«) |
| **å·²åŠ è½½** | âœ… æ˜¯ | âŒ å¦ |

**ç»“è®º**: BGE-M3 åœ¨å‡†ç¡®æ€§ã€å¤šè¯­è¨€ã€ç»´åº¦ä¸Šéƒ½æ›´ä¼˜ï¼Œè€Œä¸”ä½ å·²ç»åŠ è½½äº†ï¼Œ**ç›´æ¥ç”¨å°±è¡Œ**ï¼

---

## æµ‹è¯•éªŒè¯

### æµ‹è¯• 1: å®Œå…¨ç›¸åŒçš„é—®é¢˜ (Layer 1)

```bash
# ç¬¬ä¸€æ¬¡ - ç¼“å­˜æœªå‘½ä¸­
curl -X POST http://localhost:8888/api/rag/ask-hybrid \
  -H "Content-Type: application/json" \
  -d '{"question": "What is prop building?", "top_k": 3}'

# å“åº”:
{
  "token_usage": {"total": 1020},
  "cost": 0.00859,
  "time_ms": 2345
}

# ç¬¬äºŒæ¬¡ - Layer 1 å‘½ä¸­
curl -X POST http://localhost:8888/api/rag/ask-hybrid \
  -H "Content-Type: application/json" \
  -d '{"question": "What is prop building?", "top_k": 3}'

# å“åº”:
{
  "token_usage": {"total": 0},      # â† çœ 1020 tokens!
  "cost": 0.00,                      # â† çœ $0.0086!
  "time_ms": 0.12,                   # â† å¿« 19,000x!
  "cache_info": {
    "cache_layer": 1,
    "cache_method": "Exact Hash Match",
    "similarity": 1.0
  }
}
```

### æµ‹è¯• 2: é‡Šä¹‰é—®é¢˜ (Layer 3)

```bash
# ç¬¬ä¸‰æ¬¡ - è¯­ä¹‰ç›¸ä¼¼çš„é—®é¢˜
curl -X POST http://localhost:8888/api/rag/ask-hybrid \
  -H "Content-Type: application/json" \
  -d '{"question": "How to build props?", "top_k": 3}'

# å“åº”:
{
  "token_usage": {"total": 0},      # â† BGE-M3 è¯†åˆ«å‡ºè¯­ä¹‰ç›¸ä¼¼!
  "cost": 0.00,
  "time_ms": 7.83,                   # â† Layer 3 ç¨æ…¢ï¼Œä½†ä»å¿« 300x
  "cache_info": {
    "cache_layer": 3,
    "cache_method": "Semantic Embedding Match (BGE-M3)",
    "similarity": 0.89
  }
}
```

---

## æ€»ç»“

### âœ… ä¼˜åŠ¿

1. **å¤ç”¨ç°æœ‰æ¨¡å‹**ï¼šä¸éœ€è¦åŠ è½½ MiniLM æˆ–å…¶ä»–æ¨¡å‹
2. **BGE-M3 æ›´å¼º**ï¼š1024 ç»´å‘é‡ï¼Œè¯­ä¹‰ç†è§£æ›´å‡†ç¡®
3. **é›¶é¢å¤–æˆæœ¬**ï¼šå†…å­˜ã€è®¡ç®—ã€ç»´æŠ¤æˆæœ¬ä¸º 0
4. **å®Œç¾ä¸€è‡´æ€§**ï¼šä¸ RAG åœ¨åŒä¸€è¯­ä¹‰ç©ºé—´

### ğŸ“Š é¢„æœŸæ•ˆæœ

```
1000 æ¬¡æŸ¥è¯¢:
- Layer 1 å‘½ä¸­: 200 æ¬¡ (20%) - å®Œå…¨ç›¸åŒ
- Layer 2 å‘½ä¸­: 150 æ¬¡ (15%) - å…³é”®è¯åŒ¹é…
- Layer 3 å‘½ä¸­: 300 æ¬¡ (30%) - BGE-M3 è¯­ä¹‰åŒ¹é…
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»å‘½ä¸­ç‡: 650 æ¬¡ (65%)

èŠ‚çœ Token: 650 Ã— 1020 = 663,000 tokens
èŠ‚çœæˆæœ¬: 650 Ã— $0.0086 = $5.59
èŠ‚çœæ—¶é—´: 650 Ã— 2ç§’ = 22 åˆ†é’Ÿ
```

### ğŸš€ ä¸‹ä¸€æ­¥

å‡†å¤‡å¥½äº†å—ï¼Ÿæˆ‘å¯ä»¥ï¼š

1. âœ… ä¿®æ”¹ `enhanced_rag_pipeline.py` é›†æˆç­”æ¡ˆç¼“å­˜
2. âœ… æ·»åŠ  `/api/rag/answer-cache/stats` API endpoint
3. âœ… æ›´æ–° `.env` é…ç½®
4. âœ… æ„å»ºå¹¶æµ‹è¯•

å‘Šè¯‰æˆ‘å¼€å§‹å—ï¼Ÿ
